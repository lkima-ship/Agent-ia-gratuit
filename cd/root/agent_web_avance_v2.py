cd /root

# CrÃ©er la version amÃ©liorÃ©e de l'agent web
cat > agent_web_avance_v2.py << 'EOF'
#!/usr/bin/env python3
"""
AGENT WEB AVANCÃ‰ V2 - Scraping rÃ©el, API, Surveillance
"""
import requests
import json
import time
import os
import sys
import socket
from datetime import datetime
from urllib.parse import urlparse, urljoin, quote_plus
from bs4 import BeautifulSoup
import csv

class AgentWebAvanceV2:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.historique = []
        self.log_file = "web_agent_log.json"
        self.charger_historique()
    
    def charger_historique(self):
        """Charge l'historique des analyses"""
        if os.path.exists(self.log_file):
            try:
                with open(self.log_file, 'r') as f:
                    self.historique = json.load(f)
            except:
                self.historique = []
    
    def sauvegarder_historique(self):
        """Sauvegarde l'historique"""
        with open(self.log_file, 'w') as f:
            json.dump(self.historique[-100:], f, indent=2)
    
    def verifier_connexion(self):
        """VÃ©rifie la connexion internet"""
        tests = [
            ("Google", "https://www.google.com"),
            ("Cloudflare", "https://1.1.1.1"),
            ("OpenDNS", "https://www.opendns.com")
        ]
        
        resultats = []
        for nom, url in tests:
            try:
                start = time.time()
                response = self.session.get(url, timeout=5)
                duree = time.time() - start
                resultats.append({
                    "service": nom,
                    "statut": "âœ…" if response.status_code == 200 else "âŒ",
                    "temps": f"{duree:.2f}s",
                    "code": response.status_code
                })
            except Exception as e:
                resultats.append({
                    "service": nom,
                    "statut": "âŒ",
                    "temps": "N/A",
                    "erreur": str(e)
                })
        
        return resultats
    
    def scraper_url(self, url, profondeur=1):
        """Scrape une URL avec une profondeur donnÃ©e"""
        try:
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            print(f"\nğŸ” Scraping de {url} (profondeur: {profondeur})...")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraire les informations
            donnees = {
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "status_code": response.status_code,
                "encoding": response.encoding,
                "headers": dict(response.headers),
                "content_type": response.headers.get('content-type', ''),
                "taille": len(response.content),
                "temps_reponse": response.elapsed.total_seconds(),
                "titre": self.extraire_titre(soup),
                "meta_description": self.extraire_meta_description(soup),
                "meta_keywords": self.extraire_meta_keywords(soup),
                "langue": self.extraire_langue(soup),
                "liens": self.extraire_liens(soup, url),
                "images": self.extraire_images(soup, url),
                "textes": self.extraire_textes_importants(soup),
                "structure": self.analyser_structure(soup)
            }
            
            # Suivre les liens si profondeur > 1
            if profondeur > 1:
                donnees["liens_profond"] = []
                liens_uniques = list(set(donnees["liens"]["internes"][:5]))  # 5 premiers liens internes
                
                for lien in liens_uniques:
                    try:
                        sous_donnees = self.scraper_url(lien, profondeur-1)
                        donnees["liens_profond"].append(sous_donnees)
                    except:
                        continue
            
            # Sauvegarder dans l'historique
            self.historique.append({
                "type": "scraping",
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "donnees": donnees
            })
            self.sauvegarder_historique()
            
            return donnees
            
        except Exception as e:
            return {"erreur": str(e), "url": url}
    
    def extraire_titre(self, soup):
        """Extrait le titre de la page"""
        if soup.title:
            return soup.title.string.strip()
        return "Pas de titre"
    
    def extraire_meta_description(self, soup):
        """Extrait la meta description"""
        meta = soup.find('meta', attrs={'name': 'description'})
        if meta and meta.get('content'):
            return meta['content'].strip()[:200]
        return "Pas de description"
    
    def extraire_meta_keywords(self, soup):
        """Extrait les meta keywords"""
        meta = soup.find('meta', attrs={'name': 'keywords'})
        if meta and meta.get('content'):
            return meta['content'].strip()[:200]
        return "Pas de keywords"
    
    def extraire_langue(self, soup):
        """Extrait la langue de la page"""
        html = soup.find('html')
        if html and html.get('lang'):
            return html['lang']
        return "Non dÃ©tectÃ©"
    
    def extraire_liens(self, soup, url_base):
        """Extrait tous les liens de la page"""
        liens = soup.find_all('a', href=True)
        
        internes = []
        externes = []
        
        for lien in liens:
            href = lien['href'].strip()
            if href:
                # Normaliser l'URL
                try:
                    full_url = urljoin(url_base, href)
                    if urlparse(full_url).netloc == urlparse(url_base).netloc:
                        internes.append(full_url)
                    else:
                        externes.append(full_url)
                except:
                    continue
        
        return {
            "total": len(liens),
            "internes": list(set(internes))[:20],  # Limiter Ã  20
            "externes": list(set(externes))[:20]
        }
    
    def extraire_images(self, soup, url_base):
        """Extrait toutes les images de la page"""
        images = soup.find_all('img', src=True)
        
        donnees_images = []
        for img in images[:10]:  # Limiter Ã  10 images
            src = img.get('src', '').strip()
            alt = img.get('alt', '').strip()[:100]
            
            if src:
                try:
                    full_src = urljoin(url_base, src)
                    donnees_images.append({
                        "src": full_src,
                        "alt": alt if alt else "Pas d'alt",
                        "title": img.get('title', '')
                    })
                except:
                    continue
        
        return {
            "total": len(images),
            "images": donnees_images
        }
    
    def extraire_textes_importants(self, soup):
        """Extrait les textes importants (h1, h2, h3, p)"""
        textes = {
            "h1": [],
            "h2": [],
            "h3": [],
            "paragraphes": []
        }
        
        # Extraire les titres
        for niveau in ['h1', 'h2', 'h3']:
            for tag in soup.find_all(niveau):
                texte = tag.get_text().strip()
                if texte:
                    textes[niveau].append(texte[:200])
        
        # Extraire quelques paragraphes
        for p in soup.find_all('p')[:10]:
            texte = p.get_text().strip()
            if texte and len(texte) > 20:
                textes["paragraphes"].append(texte[:200])
        
        # Compter les mots
        tous_textes = ' '.join(textes["h1"] + textes["h2"] + textes["h3"] + textes["paragraphes"])
        mots = len(tous_textes.split())
        
        textes["statistiques"] = {
            "mots_total": mots,
            "h1_count": len(textes["h1"]),
            "h2_count": len(textes["h2"]),
            "h3_count": len(textes["h3"]),
            "paragraphes_count": len(textes["paragraphes"])
        }
        
        return textes
    
    def analyser_structure(self, soup):
        """Analyse la structure HTML de la page"""
        elements = {
            "div": len(soup.find_all('div')),
            "span": len(soup.find_all('span')),
            "table": len(soup.find_all('table')),
            "form": len(soup.find_all('form')),
            "input": len(soup.find_all('input')),
            "button": len(soup.find_all('button')),
            "script": len(soup.find_all('script')),
            "style": len(soup.find_all('style')),
            "link": len(soup.find_all('link')),
            "meta": len(soup.find_all('meta'))
        }
        
        # DÃ©tecter les frameworks
        frameworks = []
        html_str = str(soup)
        
        framework_indicators = {
            "React": ["react", "react-dom"],
            "Vue.js": ["vue", "v-app"],
            "Angular": ["ng-", "angular"],
            "jQuery": ["jquery", "$("],
            "Bootstrap": ["bootstrap", "btn btn-"],
            "Tailwind": ["tailwind", "class=.*tw-"],
            "WordPress": ["wp-", "wordpress"],
            "Joomla": ["joomla"],
            "Drupal": ["drupal"]
        }
        
        for framework, indicators in framework_indicators.items():
            for indicator in indicators:
                if indicator in html_str.lower():
                    frameworks.append(framework)
                    break
        
        return {
            "elements": elements,
            "frameworks": list(set(frameworks)),
            "doctype": self.extraire_doctype(soup)
        }
    
    def extraire_doctype(self, soup):
        """Extrait le doctype"""
        for item in soup.contents:
            if isinstance(item, str) and item.strip().startswith('<!DOCTYPE'):
                return item.strip()
        return "HTML5 (par dÃ©faut)"
    
    def tester_api(self, url, method="GET", data=None, headers=None):
        """Teste une API REST"""
        try:
            if not headers:
                headers = {
                    'Content-Type': 'application/json',
                    'Accept': 'application/json'
                }
            
            start = time.time()
            
            if method.upper() == "GET":
                response = self.session.get(url, headers=headers, timeout=10)
            elif method.upper() == "POST":
                response = self.session.post(url, json=data, headers=headers, timeout=10)
            elif method.upper() == "PUT":
                response = self.session.put(url, json=data, headers=headers, timeout=10)
            elif method.upper() == "DELETE":
                response = self.session.delete(url, headers=headers, timeout=10)
            else:
                return {"erreur": f"MÃ©thode {method} non supportÃ©e"}
            
            duree = time.time() - start
            
            resultat = {
                "url": url,
                "method": method,
                "status_code": response.status_code,
                "temps_reponse": f"{duree:.2f}s",
                "headers": dict(response.headers),
                "content_type": response.headers.get('content-type', ''),
                "taille": len(response.content)
            }
            
            # Essayer de parser la rÃ©ponse
            try:
                if 'application/json' in response.headers.get('content-type', ''):
                    resultat["body"] = response.json()
                else:
                    resultat["body_preview"] = response.text[:500]
            except:
                resultat["body_raw"] = response.text[:500]
            
            # Sauvegarder
            self.historique.append({
                "type": "api_test",
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "resultat": resultat
            })
            self.sauvegarder_historique()
            
            return resultat
            
        except Exception as e:
            return {"erreur": str(e), "url": url}
    
    def surveiller_site(self, url, interval=30, duree=300):
        """Surveille un site pendant une durÃ©e donnÃ©e"""
        print(f"\nğŸ‘ï¸ Surveillance de {url}")
        print(f"â±ï¸  Intervalle: {interval}s, DurÃ©e: {duree}s")
        print("Appuyez sur Ctrl+C pour arrÃªter")
        print("-" * 50)
        
        stats = {
            "url": url,
            "debut": datetime.now().isoformat(),
            "tests": [],
            "statistiques": {}
        }
        
        start_time = time.time()
        test_count = 0
        
        try:
            while time.time() - start_time < duree:
                test_count += 1
                print(f"\nTest #{test_count} - {datetime.now().strftime('%H:%M:%S')}")
                
                try:
                    test_start = time.time()
                    response = self.session.get(url, timeout=10)
                    test_duree = time.time() - test_start
                    
                    statut = {
                        "timestamp": datetime.now().isoformat(),
                        "status": response.status_code,
                        "temps": f"{test_duree:.2f}s",
                        "taille": len(response.content),
                        "succes": response.status_code == 200
                    }
                    
                    stats["tests"].append(statut)
                    
                    # Afficher
                    statut_symbole = "âœ…" if statut["succes"] else "âŒ"
                    print(f"{statut_symbole} Status: {statut['status']} | Temps: {statut['temps']}")
                    
                except Exception as e:
                    erreur_statut = {
                        "timestamp": datetime.now().isoformat(),
                        "erreur": str(e),
                        "succes": False
                    }
                    stats["tests"].append(erreur_statut)
                    print(f"âŒ Erreur: {e}")
                
                # Attendre l'intervalle (sauf au dernier tour)
                if time.time() - start_time + interval < duree:
                    time.sleep(interval)
                else:
                    break
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ Surveillance interrompue")
        
        # Calculer les statistiques
        if stats["tests"]:
            succes = [t for t in stats["tests"] if t.get("succes", False)]
            stats["statistiques"] = {
                "total_tests": len(stats["tests"]),
                "tests_succes": len(succes),
                "taux_succes": f"{(len(succes)/len(stats['tests'])*100):.1f}%" if stats["tests"] else "0%",
                "fin": datetime.now().isoformat(),
                "duree_totale": f"{time.time() - start_time:.1f}s"
            }
        
        # Sauvegarder
        self.historique.append({
            "type": "surveillance",
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "stats": stats
        })
        self.sauvegarder_historique()
        
        return stats
    
    def analyser_seo(self, url):
        """Analyse SEO basique d'une URL"""
        try:
            donnees = self.scraper_url(url, profondeur=1)
            
            if "erreur" in donnees:
                return donnees
            
            score = 0
            recommendations = []
            
            # VÃ©rifier le titre
            titre = donnees.get("titre", "")
            if titre and len(titre) > 10 and len(titre) < 60:
                score += 20
                recommendations.append("âœ… Titre optimisÃ© (10-60 caractÃ¨res)")
            else:
                recommendations.append("âš ï¸  Titre non optimisÃ©")
            
            # VÃ©rifier la description
            description = donnees.get("meta_description", "")
            if description and len(description) > 50 and len(description) < 160:
                score += 20
                recommendations.append("âœ… Description optimisÃ©e (50-160 caractÃ¨res)")
            else:
                recommendations.append("âš ï¸  Description manquante ou non optimisÃ©e")
            
            # VÃ©rifier les H1
            h1_count = donnees.get("textes", {}).get("statistiques", {}).get("h1_count", 0)
            if h1_count == 1:
                score += 10
                recommendations.append("âœ… 1 seul H1 (parfait)")
            elif h1_count > 1:
                recommendations.append(f"âš ï¸  {h1_count} H1 dÃ©tectÃ©s (idÃ©al: 1)")
            else:
                recommendations.append("âŒ Aucun H1 dÃ©tectÃ©")
            
            # VÃ©rifier les images
            images = donnees.get("images", {})
            images_sans_alt = [img for img in images.get("images", []) if img.get("alt") in ["", "Pas d'alt"]]
            if images_sans_alt:
                recommendations.append(f"âš ï¸  {len(images_sans_alt)} images sans attribut alt")
            else:
                score += 10
                recommendations.append("âœ… Toutes les images ont un attribut alt")
            
            # VÃ©rifier les liens
            liens = donnees.get("liens", {})
            if liens.get("total", 0) > 0:
                score += 10
                recommendations.append(f"âœ… {liens.get('total')} liens trouvÃ©s")
            
            # VÃ©rifier la structure
            structure = donnees.get("structure", {})
            if structure.get("frameworks"):
                recommendations.append(f"ğŸ› ï¸  Frameworks dÃ©tectÃ©s: {', '.join(structure['frameworks'])}")
            
            # Calcul final
            score = min(score, 100)
            
            return {
                "url": url,
                "score_seo": score,
                "note": self.obtenir_note(score),
                "recommendations": recommendations,
                "donnees_brutes": donnees
            }
            
        except Exception as e:
            return {"erreur": str(e)}
    
    def obtenir_note(self, score):
        """Convertit un score en note"""
        if score >= 90:
            return "Excellent ğŸ†"
        elif score >= 70:
            return "Bon ğŸ‘"
        elif score >= 50:
            return "Moyen âš ï¸"
        else:
            return "Ã€ amÃ©liorer ğŸš¨"
    
    def exporter_donnees(self, format_type="json"):
        """Exporte les donnÃ©es au format spÃ©cifiÃ©"""
        if not self.historique:
            return {"erreur": "Aucune donnÃ©e Ã  exporter"}
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format_type == "json":
            filename = f"export_web_{timestamp}.json"
            with open(filename, 'w') as f:
                json.dump(self.historique, f, indent=2)
            return {"succes": True, "fichier": filename}
        
        elif format_type == "csv":
            filename = f"export_web_{timestamp}.csv"
            # CrÃ©er un CSV simplifiÃ©
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(["Type", "URL", "Timestamp", "Statut"])
                
                for item in self.historique:
                    writer.writerow([
                        item.get("type", ""),
                        item.get("url", ""),
                        item.get("timestamp", ""),
                        "OK" if "erreur" not in item else "ERREUR"
                    ])
            return {"succes": True, "fichier": filename}
        
        else:
            return {"erreur": f"Format {format_type} non supportÃ©"}

def afficher_menu():
    print("\n" + "="*70)
    print("           ğŸŒ AGENT WEB AVANCÃ‰ V2 - OUTIL PROFESSIONNEL")
    print("="*70)
    print("ğŸ“‹ FONCTIONNALITÃ‰S PRINCIPALES :")
    print("1. ğŸ” Scraping web avancÃ© (analyse complÃ¨te)")
    print("2. ğŸ§ª Test d'API REST (GET, POST, PUT, DELETE)")
    print("3. ğŸ‘ï¸  Surveillance de site (temps rÃ©el)")
    print("4. ğŸ“Š Analyse SEO automatique")
    print("5. ğŸ“¡ Test de connexion internet")
    print("6. ğŸ“œ Historique des analyses")
    print("7. ğŸ’¾ Exporter les donnÃ©es (JSON/CSV)")
    print("8. âš™ï¸  ParamÃ¨tres et informations")
    print("0. ğŸšª Quitter")

def installer_dependances():
    """Installe les dÃ©pendances nÃ©cessaires"""
    print("\nğŸ“¦ VÃ©rification des dÃ©pendances...")
    
    try:
        import requests
        print("âœ… requests dÃ©jÃ  installÃ©")
    except ImportError:
        print("ğŸ“¦ Installation de requests...")
        os.system(f"{sys.executable} -m pip install requests --quiet")
    
    try:
        from bs4 import BeautifulSoup
        print("âœ… beautifulsoup4 dÃ©jÃ  installÃ©")
    except ImportError:
        print("ğŸ“¦ Installation de beautifulsoup4...")
        os.system(f"{sys.executable} -m pip install beautifulsoup4 --quiet")
    
    print("âœ… Toutes les dÃ©pendances sont prÃªtes !")

def main():
    # Installer les dÃ©pendances si nÃ©cessaire
    installer_dependances()
    
    agent = AgentWebAvanceV2()
    
    print("ğŸš€ LANCEMENT DE L'AGENT WEB AVANCÃ‰ V2")
    print("Version 2.0 - Outil professionnel de scraping et analyse web")
    
    # Tester la connexion
    print("\nğŸ“¡ Test de connexion rapide...")
    tests_connexion = agent.verifier_connexion()
    connexion_ok = any(t["statut"] == "âœ…" for t in tests_connexion)
    
    if connexion_ok:
        print("âœ… ConnectÃ© Ã  internet")
    else:
        print("âš ï¸  Mode hors ligne - certaines fonctionnalitÃ©s seront limitÃ©es")
    
    while True:
        afficher_menu()
        
        choix = input("\nğŸ‘‰ Votre choix : ")
        
        if choix == "1":
            url = input("\nğŸŒ URL Ã  scraper : ").strip()
            if not url:
                print("âŒ URL vide")
                continue
            
            profondeur = input("Profondeur de scraping (1-3, dÃ©faut: 1) : ").strip()
            if not profondeur:
                profondeur = 1
            else:
                try:
                    profondeur = int(profondeur)
                    if profondeur < 1 or profondeur > 3:
                        print("âš ï¸  Profondeur limitÃ©e Ã  3, utilisation de 1")
                        profondeur = 1
                except:
                    profondeur = 1
            
            print(f"\nğŸ” DÃ©but du scraping...")
            resultats = agent.scraper_url(url, profondeur)
            
            if "erreur" in resultats:
                print(f"âŒ Erreur : {resultats['erreur']}")
            else:
                print(f"\nâœ… SCRAPING RÃ‰USSI !")
                print(f"ğŸ“ URL : {resultats['url']}")
                print(f"ğŸ“Š Statut : {resultats['status_code']}")
                print(f"â±ï¸  Temps de rÃ©ponse : {resultats['temps_reponse']:.2f}s")
                print(f"ğŸ“ Taille : {resultats['taille']} octets")
                
                print(f"\nğŸ“ TITRE : {resultats['titre']}")
                print(f"ğŸ“„ DESCRIPTION : {resultats['meta_description']}")
                print(f"ğŸŒ LANGUE : {resultats['langue']}")
                
                print(f"\nğŸ”— LIENS : {resultats['liens']['total']} total")
                print(f"   â€¢ Internes : {len(resultats['liens']['internes'])}")
                print(f"   â€¢ Externes : {len(resultats['liens']['externes'])}")
                
                print(f"\nğŸ–¼ï¸  IMAGES : {resultats['images']['total']} images")
                
                print(f"\nğŸ“Š STRUCTURE :")
                for elem, count in resultats['structure']['elements'].items():
                    if count > 0:
                        print(f"   â€¢ {elem} : {count}")
                
                if resultats['structure']['frameworks']:
                    print(f"ğŸ› ï¸  FRAMEWORKS : {', '.join(resultats['structure']['frameworks'])}")
        
        elif choix == "2":
            print("\nğŸ§ª TEST D'API REST")
            url = input("URL de l'API : ").strip()
            if not url:
                print("âŒ URL vide")
                continue
            
            method = input("MÃ©thode (GET/POST/PUT/DELETE, dÃ©faut: GET) : ").strip().upper()
            if method not in ["GET", "POST", "PUT", "DELETE"]:
                method = "GET"
            
            data = None
            if method in ["POST", "PUT"]:
                data_input = input("DonnÃ©es JSON (optionnel, ex: {\"key\":\"value\"}) : ").strip()
                if data_input:
                    try:
                        data = json.loads(data_input)
                    except:
                        print("âš ï¸  JSON invalide, utilisation sans donnÃ©es")
            
            print(f"\nğŸ” Test de l'API avec mÃ©thode {method}...")
            resultats = agent.tester_api(url, method, data)
            
            if "erreur" in resultats:
                print(f"âŒ Erreur : {resultats['erreur']}")
            else:
                print(f"\nâœ… TEST API RÃ‰USSI !")
                print(f"ğŸ“¡ URL : {resultats['url']}")
                print(f"âš¡ MÃ©thode : {resultats['method']}")
                print(f"ğŸ“Š Statut : {resultats['status_code']}")
                print(f"â±ï¸  Temps de rÃ©ponse : {resultats['temps_reponse']}")
                print(f"ğŸ“ Taille : {resultats['taille']} octets")
                print(f"ğŸ“„ Type de contenu : {resultats['content_type']}")
                
                if "body" in resultats:
                    print(f"\nğŸ“¦ RÃ‰PONSE JSON :")
                    print(f"   {json.dumps(resultats['body'], indent=2)[:200]}...")
                elif "body_preview" in resultats:
                    print(f"\nğŸ“„ PRÃ‰VISUALISATION RÃ‰PONSE :")
                    print(f"   {resultats['body_preview'][:200]}...")
        
        elif choix == "3":
            print("\nğŸ‘ï¸  SURVEILLANCE DE SITE")
            url = input("URL Ã  surveiller : ").strip()
            if not url:
                print("âŒ URL vide")
                continue
            
            interval = input("Intervalle en secondes (dÃ©faut: 30) : ").strip()
            if not interval:
                interval = 30
            else:
                try:
                    interval = int(interval)
                except:
                    interval = 30
            
            duree = input("DurÃ©e totale en secondes (dÃ©faut: 300) : ").strip()
            if not duree:
                duree = 300
            else:
                try:
                    duree = int(duree)
                except:
                    duree = 300
            
            print(f"\nğŸš€ Lancement de la surveillance...")
            stats = agent.surveiller_site(url, interval, duree)
            
            print(f"\nğŸ“Š RAPPORT DE SURVEILLANCE :")
            print(f"ğŸ“ URL : {stats['url']}")
            print(f"â±ï¸  DÃ©but : {stats['debut']}")
            print(f"ğŸ“ˆ Fin : {stats['statistiques'].get('fin', 'N/A')}")
            print(f"ğŸ”„ Total tests : {stats['statistiques'].get('total_tests', 0)}")
            print(f"âœ… Tests rÃ©ussis : {stats['statistiques'].get('tests_succes', 0)}")
            print(f"ğŸ“Š Taux de rÃ©ussite : {stats['statistiques'].get('taux_succes', '0%')}")
            print(f"â³ DurÃ©e totale : {stats['statistiques'].get('duree_totale', '0s')}")
        
        elif choix == "4":
            url = input("\nğŸ” URL Ã  analyser (SEO) : ").strip()
            if not url:
                print("âŒ URL vide")
                continue
            
            print(f"\nğŸ“Š Analyse SEO en cours...")
            analyse = agent.analyser_seo(url)
            
            if "erreur" in analyse:
                print(f"âŒ Erreur : {analyse['erreur']}")
            else:
                print(f"\nğŸ“ˆ RAPPORT SEO COMPLET")
                print(f"ğŸŒ URL : {analyse['url']}")
                print(f"ğŸ† SCORE : {analyse['score_seo']}/100")
                print(f"ğŸ“Š NOTE : {analyse['note']}")
                
                print(f"\nğŸ’¡ RECOMMANDATIONS :")
                for rec in analyse['recommendations']:
                    print(f"   â€¢ {rec}")
                
                # Afficher quelques dÃ©tails
                donnees = analyse.get('donnees_brutes', {})
                if donnees:
                    print(f"\nğŸ“ INFORMATIONS DÃ‰TECTÃ‰ES :")
                    print(f"   â€¢ Titre : {donnees.get('titre', 'N/A')[:50]}...")
                    print(f"   â€¢ Description : {donnees.get('meta_description', 'N/A')[:50]}...")
                    print(f"   â€¢ Images : {donnees.get('images', {}).get('total', 0)}")
                    print(f"   â€¢ Liens : {donnees.get('liens', {}).get('total', 0)}")
        
        elif choix == "5":
            print("\nğŸ“¡ TEST DE CONNEXION INTERNET")
            resultats = agent.verifier_connexion()
            
            print(f"\nğŸ”Œ RÃ‰SULTATS DES TESTS :")
            for test in resultats:
                print(f"   â€¢ {test['service']} : {test['statut']} ({test.get('temps', 'N/A')})")
            
            # RÃ©sumÃ©
            succes = sum(1 for t in resultats if t["statut"] == "âœ…")
            print(f"\nğŸ“Š RÃ‰SUMÃ‰ : {succes}/{len(resultats)} services accessibles")
        
        elif choix == "6":
            if not agent.historique:
                print("\nğŸ“­ Historique vide")
            else:
                print(f"\nğŸ“œ HISTORIQUE DES ANALYSES ({len(agent.historique)})")
                print("-" * 60)
                
                for i, item in enumerate(agent.historique[-10:], 1):
                    date = datetime.fromisoformat(item['timestamp']).strftime('%H:%M')
                    type_emoji = {
                        "scraping": "ğŸ”",
                        "api_test": "ğŸ§ª",
                        "surveillance": "ğŸ‘ï¸"
                    }.get(item.get('type', ''), 'ğŸ“„')
                    
                    print(f"{i}. [{date}] {type_emoji} {item.get('type', 'inconnu').upper()}")
                    print(f"   ğŸ“ {item.get('url', 'N/A')[:50]}...")
                    
                    if "erreur" in item:
                        print(f"   âŒ Erreur")
                    else:
                        print(f"   âœ… SuccÃ¨s")
                    
                    print()
        
        elif choix == "7":
            print("\nğŸ’¾ EXPORTATION DES DONNÃ‰ES")
            print("1. Format JSON (recommandÃ©)")
            print("2. Format CSV (simple)")
            
            format_choix = input("Format : ").strip()
            
            if format_choix == "1":
                resultat = agent.exporter_donnees("json")
            elif format_choix == "2":
                resultat = agent.exporter_donnees("csv")
            else:
                print("âŒ Choix invalide")
                continue
            
            if "erreur" in resultat:
                print(f"âŒ {resultat['erreur']}")
            else:
                print(f"âœ… DonnÃ©es exportÃ©es avec succÃ¨s !")
                print(f"ğŸ“ Fichier : {resultat['fichier']}")
                print(f"ğŸ“Š {len(agent.historique)} entrÃ©es exportÃ©es")
        
        elif choix == "8":
            print("\nâš™ï¸  INFORMATIONS SUR L'AGENT :")
            print(f"Version : 2.0 (Web AvancÃ©)")
            print(f"Analyses enregistrÃ©es : {len(agent.historique)}")
            print(f"Fichier de logs : {agent.log_file}")
            print(f"User-Agent : {agent.session.headers['User-Agent'][:50]}...")
            
            # Tests de connexion rapides
            print(f"\nğŸ“¡ Ã‰TAT CONNEXION :")
            tests = agent.verifier_connexion()
            for test in tests:
                print(f"   â€¢ {test['service']} : {test['statut']}")
        
        elif choix == "0":
            print("\nğŸ‘‹ Au revoir ! Agent web terminÃ©.")
            if agent.historique:
                print(f"ğŸ“Š RÃ©sumÃ© : {len(agent.historique)} analyses enregistrÃ©es")
            break
        
        else:
            print("âŒ Choix invalide")
        
        input("\nâ†µ Appuyez sur EntrÃ©e pour continuer...")

if __name__ == "__main__":
    main()
EOF

# Rendre exÃ©cutable
chmod +x agent_web_avance_v2.py

# Tester l'agent
python3 agent_web_avance_v2.py
